{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02a79deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import statistics\n",
    "import math\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824ee004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/xlcq7dfs5hb1cq7xjkkmbzr40000gn/T/ipykernel_89905/2393232715.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  k20 = pd.read_csv( \"kaggle2020.csv\" )\n",
      "/var/folders/l_/xlcq7dfs5hb1cq7xjkkmbzr40000gn/T/ipykernel_89905/2393232715.py:3: DtypeWarning: Columns (0,195,201,285,286,287,288,289,290,291,292) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  k21 = pd.read_csv( \"kaggle2021.csv\" )\n",
      "/var/folders/l_/xlcq7dfs5hb1cq7xjkkmbzr40000gn/T/ipykernel_89905/2393232715.py:5: DtypeWarning: Columns (0,208,225,255,257,260,270,271,277) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  k22 = pd.read_csv( \"kaggle2022.csv\" )\n"
     ]
    }
   ],
   "source": [
    "k20 = pd.read_csv( \"kaggle2020.csv\" )\n",
    "k20 = pd.DataFrame(k20)\n",
    "k21 = pd.read_csv( \"kaggle2021.csv\" )\n",
    "k21 = pd.DataFrame(k21)\n",
    "k22 = pd.read_csv( \"kaggle2022.csv\" )\n",
    "k22 = pd.DataFrame(k22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "194fef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70008, 33)\n"
     ]
    }
   ],
   "source": [
    "#filter necessary columns, create new column to indicate year for each dataframe\n",
    "new20 = k20[['Q1','Q2','Q24', 'Q3', 'Q6','Q4','Q5','Q7_Part_1','Q7_Part_2','Q7_Part_3','Q7_Part_4','Q7_Part_5','Q7_Part_6','Q7_Part_7','Q7_Part_8','Q7_Part_9', 'Q7_Part_10','Q7_Part_11','Q7_Part_12','Q7_OTHER', 'Q14_Part_1','Q14_Part_2','Q14_Part_3','Q14_Part_4','Q14_Part_5','Q14_Part_6','Q14_Part_7','Q14_Part_8','Q14_Part_9','Q14_Part_10','Q14_OTHER','Q15', 'Q22','Q25']]\n",
    "new20.insert(0, \"Survey Year\", 2020)\n",
    "new21 = k21[['Q1','Q2','Q25','Q3', 'Q6','Q4','Q5','Q7_Part_1','Q7_Part_2','Q7_Part_3','Q7_Part_4','Q7_Part_5','Q7_Part_6','Q7_Part_7','Q7_Part_8','Q7_Part_9', 'Q7_Part_10','Q7_Part_11','Q7_Part_12','Q7_OTHER', 'Q14_Part_1','Q14_Part_2','Q14_Part_3','Q14_Part_4','Q14_Part_5','Q14_Part_6','Q14_Part_7','Q14_Part_8','Q14_Part_9','Q14_Part_10','Q14_OTHER','Q15', 'Q23','Q26']]\n",
    "new21.insert(0, \"Survey Year\", 2021)\n",
    "new22 = k22[['Q2','Q3','Q29', 'Q4', 'Q11','Q8','Q23','Q12_1','Q12_2','Q12_3','Q12_4','Q12_5','Q12_6','Q12_7','Q12_8','Q12_9','Q12_10', 'Q12_11','Q12_12','Q12_13','Q12_14','Q12_15', 'Q15_1', 'Q15_2','Q15_3','Q15_4','Q15_5','Q15_6','Q15_7','Q15_8','Q15_9','Q15_10', 'Q15_15','Q16', 'Q27', 'Q30']]\n",
    "new22.insert(0, \"Survey Year\", 2022)\n",
    "\n",
    "#2020 and 2021 does not include C# and GO options, while 2022 does not include Swift option, \n",
    "# sub these options into the “Others” category\n",
    "current = 0\n",
    "for row in new22.itertuples():\n",
    "    a = row.Q12_5 \n",
    "    b = row.Q12_13 \n",
    "    if a == 'C#' or b =='Go':\n",
    "        new22.loc[current,'Q12_15'] = 'Other'\n",
    "    current += 1\n",
    "current = 0\n",
    "for row in new21.itertuples():\n",
    "    a = row.Q7_Part_9  \n",
    "    if a == 'Swift':\n",
    "        new21.loc[current,'Q7_OTHER'] = 'Other'\n",
    "    current += 1\n",
    "current = 0\n",
    "for row in new20.itertuples():\n",
    "    a = row.Q7_Part_9  \n",
    "    if a == 'Swift':\n",
    "        new20.loc[current,'Q7_OTHER'] = 'Other'\n",
    "    current += 1\n",
    "    \n",
    "#remove certain columns\n",
    "new20 = new20[['Survey Year','Q1','Q2','Q24', 'Q3', 'Q6','Q4','Q5','Q7_Part_1','Q7_Part_2','Q7_Part_3','Q7_Part_4','Q7_Part_5','Q7_Part_6','Q7_Part_7','Q7_Part_8', 'Q7_Part_10','Q7_Part_11','Q7_OTHER', 'Q14_Part_1','Q14_Part_2','Q14_Part_3','Q14_Part_4','Q14_Part_5','Q14_Part_6','Q14_Part_7','Q14_Part_8','Q14_Part_9','Q14_Part_10','Q14_OTHER','Q15', 'Q22', 'Q25']]\n",
    "new21 = new21[['Survey Year','Q1','Q2','Q25','Q3', 'Q6','Q4','Q5','Q7_Part_1','Q7_Part_2','Q7_Part_3','Q7_Part_4','Q7_Part_5','Q7_Part_6','Q7_Part_7','Q7_Part_8', 'Q7_Part_10','Q7_Part_11','Q7_OTHER', 'Q14_Part_1','Q14_Part_2','Q14_Part_3','Q14_Part_4','Q14_Part_5','Q14_Part_6','Q14_Part_7','Q14_Part_8','Q14_Part_9','Q14_Part_10','Q14_OTHER','Q15', 'Q23','Q26']]\n",
    "new22 = new22[['Survey Year','Q2','Q3','Q29', 'Q4', 'Q11','Q8','Q23','Q12_1','Q12_2','Q12_3','Q12_4','Q12_6','Q12_7','Q12_8','Q12_12','Q12_9', 'Q12_11','Q12_15', 'Q15_1', 'Q15_2','Q15_3','Q15_4','Q15_5','Q15_6','Q15_7','Q15_8','Q15_9','Q15_10','Q15_15','Q16','Q27','Q30']]\n",
    "\n",
    "#rename 2022 columns so that all columns names are the same\n",
    "new20 = new20.rename(columns={'Survey Year':'Year','Q1':'Age','Q2':'Gender','Q24':'Income','Q3':'Country','Q6':'Experience','Q4':'Education','Q5':'Job Role','Q7_Part_1':'Python','Q7_Part_2':'R','Q7_Part_3':'SQL','Q7_Part_4':'C','Q7_Part_5':'C++','Q7_Part_6':'Java','Q7_Part_7':'Javascript','Q7_Part_8':'Julia','Q7_Part_10':'Bash','Q7_Part_11':'MATLAB','Q7_OTHER':'Other Lang', 'Q14_Part_1':'Matplotlib', 'Q14_Part_2':'Seaborn','Q14_Part_3':'Ploty','Q14_Part_4':'Ggplot','Q14_Part_5':'Shiny','Q14_Part_6':'D3','Q14_Part_7':'Altair','Q14_Part_8':'Bokeh','Q14_Part_9':'Geoplotlib','Q14_Part_10':'Leaflet','Q14_OTHER':'Other Vis','Q15':'ML Years','Q22':'ML in Job','Q25':'Spending on ML'})\n",
    "new21 = new21.rename(columns={'Survey Year':'Year','Q1':'Age','Q2':'Gender','Q25':'Income','Q3':'Country','Q6':'Experience','Q4':'Education','Q5':'Job Role','Q7_Part_1':'Python','Q7_Part_2':'R','Q7_Part_3':'SQL','Q7_Part_4':'C','Q7_Part_5':'C++','Q7_Part_6':'Java','Q7_Part_7':'Javascript','Q7_Part_8':'Julia','Q7_Part_10':'Bash','Q7_Part_11':'MATLAB','Q7_OTHER':'Other Lang', 'Q14_Part_1':'Matplotlib', 'Q14_Part_2':'Seaborn','Q14_Part_3':'Ploty','Q14_Part_4':'Ggplot','Q14_Part_5':'Shiny','Q14_Part_6':'D3','Q14_Part_7':'Altair','Q14_Part_8':'Bokeh','Q14_Part_9':'Geoplotlib','Q14_Part_10':'Leaflet','Q14_OTHER':'Other Vis','Q15':'ML Years','Q23':'ML in Job','Q26':'Spending on ML'})\n",
    "new22 = new22.rename(columns={'Survey Year':'Year','Q2':'Age','Q3':'Gender','Q29':'Income','Q4':'Country','Q11':'Experience','Q8':'Education','Q23':'Job Role','Q12_1':'Python','Q12_2':'R','Q12_3':'SQL','Q12_4':'C','Q12_6':'C++','Q12_7':'Java','Q12_8':'Javascript','Q12_12':'Julia','Q12_9':'Bash', 'Q12_11':'MATLAB','Q12_15':'Other Lang','Q15_1':'Matplotlib', 'Q15_2':'Seaborn','Q15_3':'Ploty','Q15_4':'Ggplot','Q15_5':'Shiny','Q15_6':'D3','Q15_7':'Altair','Q15_8':'Bokeh','Q15_9':'Geoplotlib','Q15_10':'Leaflet','Q15_15':'Other Vis','Q16':'ML Years', 'Q27':'ML in Job','Q30':'Spending on ML'})\n",
    "\n",
    "\n",
    "#concat the 3 dataframes together\n",
    "\n",
    "data2 = pd.concat([new20,new21])\n",
    "data = pd.concat([data2,new22])\n",
    "\n",
    "data = data.tail(-1)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "#group certain age categories together \n",
    "data['Age'] = data['Age'].str.replace(\"18-21\",\"18-24\")\n",
    "data['Age'] = data['Age'].str.replace(\"22-24\",\"18-24\")\n",
    "#group certain age categories together \n",
    "data['Experience'] = data['Experience'].str.replace(\"1-2 years\",\"1-3 years\")\n",
    "#group certain role categories together \n",
    "data['Job Role'] = data['Job Role'].str.replace(\"Product/Project Manager\",\"Manager\")\n",
    "data['Job Role'] = data['Job Role'].str.replace(\"Program/Project Manager\",\"Manager\")\n",
    "data['Job Role'] = data['Job Role'].str.replace(\"Product Manager\",\"Manager\")\n",
    "data['Job Role'] = data['Job Role'].str.replace(\"Manager (Program, Project, Operations, Executive-level, etc)\",\"Manager\", regex=False)\n",
    "data['Job Role'] = data['Job Role'].str.replace(\"Machine Learning Engineer\",\"ML Engineer\")\n",
    "data['Job Role'] = data['Job Role'].str.replace(\"Machine Learning/ MLops Engineer\",\"ML Engineer\")\n",
    "data['Job Role'] = data['Job Role'].str.replace(\"Data Analyst (Business, Marketing, Financial, Quantitative, etc)\",\"Data Analyst\", regex=False)\n",
    "data['Job Role'] = data['Job Role'].str.replace(\"Developer Advocate\",\"Developer Relations/Advocacy\")\n",
    "data['Income'] = data['Income'].str.replace(\"$500,000-999,999\",\"> $500,000\", regex=False)\n",
    "data['Income'] = data['Income'].str.replace(\">$1,000,000\",\"> $500,000\", regex=False)\n",
    "data['Income'] = data['Income'].str.replace(\"300,000-499,999\",\"300,000-500,000\", regex=False)\n",
    "\n",
    "#replacing other minority gender categories as \"Others\"\n",
    "data['Gender'] = data['Gender'].replace(['Prefer not to say', 'Nonbinary', 'Prefer to self-describe'], 'Others')\n",
    "\n",
    "\n",
    "data = data.drop(data[data['Job Role'] == 'Select the title most similar to your current role (or most recent title if retired): - Selected Choice'].index)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cd089f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping data points with irrelevant job roles\n",
    "data = data.dropna(subset=['Job Role'])\n",
    "data = data.drop(data[data['Job Role']=='Student'].index)\n",
    "data = data.drop(data[data['Job Role']=='Currently not employed'].index)\n",
    "data = data.drop(data[data['Job Role']=='Other'].index)\n",
    "\n",
    "unique_job_roles = data['Job Role'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e89b47e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_list = ['Python', 'R', 'SQL', 'C' , 'C++', 'Java', 'Javascript', 'Julia', 'Bash', 'MATLAB', 'Other Lang']\n",
    "\n",
    "country_list_all_data = data['Country']\n",
    "unique_country_list = country_list_all_data.unique()\n",
    "\n",
    "dv_tools_list = ['Matplotlib', 'Seaborn','Ploty','Ggplot','Shiny','D3','Altair','Bokeh','Geoplotlib','Leaflet','Other Vis']\n",
    "\n",
    "gender_list = data['Gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da7535f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the categorical variables into numerical variables\n",
    "class IncomeTransformer:\n",
    "    def fit(self, df, y = None, **fit_params):\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df, y = None, **fit_params):\n",
    "        \n",
    "        proportions = df['Income'].value_counts(normalize=True)\n",
    "        unique_income_ranges = proportions.index\n",
    "        df['Income'] = df['Income'].fillna(pd.Series(np.random.choice(unique_income_ranges,p=proportions,size=len(df))))\n",
    "        \n",
    "        OHE_Income = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            row_income_str = row['Income']\n",
    "            if(row_income_str == \"$0-999\"):\n",
    "                OHE_Income.append(500)\n",
    "            elif(row_income_str == \"> $500,000\"):\n",
    "                OHE_Income.append(500000)\n",
    "            else:\n",
    "                row_income_str = row_income_str.replace(\",\",\"\")\n",
    "                row_income_str = row_income_str.strip()\n",
    "                lower, upper = row_income_str.split(\"-\")\n",
    "                mean_income = (float(lower)+float(upper))/2\n",
    "                OHE_Income.append(mean_income)\n",
    "                \n",
    "        OHE_Income = np.array(OHE_Income)\n",
    "        \n",
    "        min_val = np.min(OHE_Income)\n",
    "        max_val = np.max(OHE_Income)\n",
    "        print(\"Min Income \",min_val)\n",
    "        print(\"Max Income \",max_val)\n",
    "        \n",
    "        OHE_Income = (OHE_Income - min_val) / (max_val - min_val)\n",
    "        \n",
    "        OHE_Income = OHE_Income.reshape(-1,1)\n",
    "                \n",
    "        return OHE_Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a12c84e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binning the experience to numerical mean in the range\n",
    "class ExpTransformer:\n",
    "    def fit(self, df, y = None, **fit_params):\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df, y = None, **fit_params):\n",
    "        \n",
    "        #print(df['Experience'].value_counts(dropna=False))\n",
    "        \n",
    "        proportions = df['Experience'].value_counts(normalize=True)\n",
    "        unique_age_ranges = proportions.index\n",
    "        df['Experience'] = df['Experience'].fillna(pd.Series(np.random.choice(unique_age_ranges,p=proportions,size=len(df))))\n",
    "\n",
    "        #print(df['Experience'].value_counts(dropna=False))\n",
    "        \n",
    "        OHE_Exp = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            row_exp_str = row['Experience']\n",
    "            if(row_exp_str == \"I have never written code\"):\n",
    "                OHE_Exp.append(0)\n",
    "            elif(row_exp_str == \"< 1 years\"):\n",
    "                OHE_Exp.append(0.5)\n",
    "            elif(row_exp_str == \"20+ years\"):\n",
    "                OHE_Exp.append(20)\n",
    "            else:\n",
    "                row_exp_str = row_exp_str.replace(\"years\",\"\")\n",
    "                row_exp_str = row_exp_str.strip()\n",
    "                lower, upper = row_exp_str.split(\"-\")\n",
    "                mean_exp = (float(lower)+float(upper))/2\n",
    "                OHE_Exp.append(mean_exp)\n",
    "                \n",
    "        OHE_Exp = np.array(OHE_Exp)\n",
    "        OHE_Exp = OHE_Exp.reshape(-1,1)\n",
    "        \n",
    "        print(\"Shape of the OHE_Exp: \", OHE_Exp.shape)\n",
    "                \n",
    "        return OHE_Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c4610bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binning the age values to numerical mean in the range\n",
    "class AgeTransformer:\n",
    "    def fit(self, df, y = None, **fit_params):\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df, y = None, **fit_params):\n",
    "        \n",
    "        OHE_Age = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            row_age_str = row['Age']\n",
    "            if(row_age_str == \"70+\"):\n",
    "                OHE_Age.append(75)\n",
    "            else:\n",
    "                lower, upper = row_age_str.split(\"-\")\n",
    "                mean_age = (float(lower)+float(upper))/2\n",
    "                OHE_Age.append(mean_age)\n",
    "                \n",
    "        OHE_Age = np.array(OHE_Age)\n",
    "        \n",
    "        min_val = np.min(OHE_Age)\n",
    "        max_val = np.max(OHE_Age)\n",
    "        OHE_Age = (OHE_Age - min_val) / (max_val - min_val)\n",
    "        \n",
    "        OHE_Age = OHE_Age.reshape(-1,1)\n",
    "        \n",
    "        print(\"Shape of the OHE_Age: \", OHE_Age.shape)\n",
    "        \n",
    "        print(min_val)\n",
    "        print(max_val)\n",
    "                \n",
    "        return OHE_Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc6a483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordinal encoding for education column in data\n",
    "class EduTransformer:\n",
    "    def fit(self, df, y = None, **fit_params):\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df, y = None, **fit_params):\n",
    "        \n",
    "        OHE_Edu = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            if(df.at[index,'Education'] == \"I prefer not to answer\"):\n",
    "                OHE_Edu.append(0)\n",
    "            elif(df.at[index,'Education'] == \"No formal education past high school\"):\n",
    "                OHE_Edu.append(1)\n",
    "            elif(df.at[index,'Education'] == \"Some college/university study without earning a bachelor’s degree\"):\n",
    "                OHE_Edu.append(2)\n",
    "            elif(df.at[index,'Education'] == \"Bachelor’s degree\"):\n",
    "                OHE_Edu.append(3)\n",
    "            elif(df.at[index,'Education'] == \"Master’s degree\"):\n",
    "                OHE_Edu.append(4)\n",
    "            elif(df.at[index,'Education'] == \"Professional degree\"):\n",
    "                OHE_Edu.append(4)\n",
    "            elif(df.at[index,'Education'] == \"Doctoral degree\"):\n",
    "                OHE_Edu.append(5)\n",
    "            elif(df.at[index,'Education'] == \"Professional doctorate\"):\n",
    "                OHE_Edu.append(6)\n",
    "                \n",
    "        OHE_Edu = np.array(OHE_Edu)\n",
    "        OHE_Edu = OHE_Edu.reshape(-1,1)\n",
    "        \n",
    "        print(\"Shape of the OHE_Edu: \", OHE_Edu.shape)\n",
    "        \n",
    "        return OHE_Edu\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "515fb0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the Known_PL\n",
    "\n",
    "class PLTransformer:\n",
    "    def fit(self, df, y = None, **fit_params):\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df, y = None, **fit_params):\n",
    "        \n",
    "        OHE_PL = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            row_PL = []\n",
    "            for item in pl_list:\n",
    "                if(pd.isna(df.at[index,item])):\n",
    "                    row_PL.append(0)\n",
    "                else:\n",
    "                    row_PL.append(1)\n",
    "            OHE_PL.append(row_PL)\n",
    "            \n",
    "        OHE_PL = np.array(OHE_PL)\n",
    "        \n",
    "        print(\"Shape of the OHE_PL: \", OHE_PL.shape)\n",
    "                \n",
    "        return OHE_PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43a0a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the countries\n",
    "\n",
    "class CountryTransformer:  \n",
    "    def fit(self, df, y = None, **fit_params):\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df, y = None, **fit_params):\n",
    "        nrows = len(df)\n",
    "        ncolumns = len(unique_country_list)\n",
    "        \n",
    "        df_Country = pd.DataFrame(0, index=range(nrows), columns=unique_country_list)\n",
    "        OHE_Country = []\n",
    "        \n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            row_Country = row['Country']\n",
    "            df_Country.at[index,row_Country] = 1\n",
    "            \n",
    "        OHE_Country = df_Country.to_numpy()\n",
    "        \n",
    "        print(\"Shape of the OHE_Country: \", OHE_Country.shape)\n",
    "        \n",
    "        return OHE_Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf7197a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the Data Visualisation tools\n",
    "\n",
    "class DataVisTransformer:  \n",
    "    def fit(self, df, y = None, **fit_params):\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df, y = None, **fit_params):\n",
    "        \n",
    "        OHE_DV = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            row_DV_count = 0\n",
    "            for item in dv_tools_list:\n",
    "                if not (pd.isna(df.at[index,item])):\n",
    "                    row_DV_count = row_DV_count + 1\n",
    "                \n",
    "            OHE_DV.append(row_DV_count)\n",
    "            \n",
    "        OHE_DV = np.array(OHE_DV)\n",
    "        OHE_DV = OHE_DV.reshape(-1,1)\n",
    "        \n",
    "        print(\"Shape of the OHE_DV: \", OHE_DV.shape)\n",
    "                \n",
    "        return OHE_DV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89699fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the ML Years\n",
    "\n",
    "class MLYearsTransformer:\n",
    "    def fit(self, df, y = None, **fit_params):\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df, y = None, **fit_params):\n",
    "        \n",
    "        #print(df['ML Years'].value_counts(dropna=False))\n",
    "        \n",
    "        proportions = df['ML Years'].value_counts(normalize=True)\n",
    "        unique_ml_year_ranges = proportions.index\n",
    "        df['ML Years'] = df['ML Years'].fillna(pd.Series(np.random.choice(unique_ml_year_ranges,p=proportions,size=len(df))))\n",
    "        \n",
    "        #print(unique_ml_year_ranges)\n",
    "        #print(df['ML Years'].value_counts(dropna=False))\n",
    "        \n",
    "        OHE_MLY = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            row_MLY_str = row['ML Years']\n",
    "            if(row_MLY_str == \"I do not use machine learning methods\"):\n",
    "                OHE_MLY.append(0)\n",
    "            elif(row_MLY_str == \"Under 1 year\"):\n",
    "                OHE_MLY.append(0.5)\n",
    "            elif(row_MLY_str == \"20 or more years\"):\n",
    "                OHE_MLY.append(20)\n",
    "            else:\n",
    "                #print(row_MLY_str)\n",
    "                row_MLY_str = row_MLY_str.replace(\"years\",\"\")\n",
    "                row_MLY_str = row_MLY_str.strip()\n",
    "                lower, upper = row_MLY_str.split(\"-\")\n",
    "                mean_exp = (float(lower)+float(upper))/2\n",
    "                OHE_MLY.append(mean_exp)\n",
    "                \n",
    "        OHE_MLY = np.array(OHE_MLY)\n",
    "        \n",
    "        min_val = np.min(OHE_MLY)\n",
    "        max_val = np.max(OHE_MLY)\n",
    "        OHE_MLY = (OHE_MLY - min_val) / (max_val - min_val)\n",
    "        \n",
    "        print(min_val)\n",
    "        print(max_val)\n",
    "        \n",
    "        OHE_MLY = OHE_MLY.reshape(-1,1)\n",
    "        \n",
    "        print(\"Shape of the OHE_MLY: \", OHE_MLY.shape)\n",
    "                \n",
    "        return OHE_MLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07e99e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Gender\n",
    "\n",
    "class GenderTransformer:  \n",
    "    def fit(self, df, y = None, **fit_params):\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df, y = None, **fit_params):\n",
    "        nrows = len(df)\n",
    "        \n",
    "        df_Gender = pd.DataFrame(0, index=range(nrows), columns=gender_list)\n",
    "        OHE_Gender = []\n",
    "        \n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            row_Gender = row['Gender']\n",
    "            df_Gender.at[index,row_Gender] = 1\n",
    "            \n",
    "        OHE_Gender = df_Gender.to_numpy()\n",
    "        \n",
    "        print(\"Shape of the OHE_Country: \", OHE_Gender.shape)\n",
    "        \n",
    "        return OHE_Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27aeb216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordinal encoding for ML incoporation in job\n",
    "class MLinJobTransformer:\n",
    "    def fit(self, df, y = None, **fit_params):\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df, y = None, **fit_params):\n",
    "        \n",
    "        proportions = df['ML in Job'].value_counts(normalize=True)\n",
    "        unique_mlinjob_ranges = proportions.index\n",
    "        df['ML in Job'] = df['ML in Job'].fillna(pd.Series(np.random.choice(unique_mlinjob_ranges,p=proportions,size=len(df))))\n",
    "        \n",
    "        OHE_MLinJob = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            row_MLinJob = row['ML in Job']\n",
    "            #print(row_MLinJob)\n",
    "            row_MLinJob = row_MLinJob.strip()\n",
    "            if(row_MLinJob == \"I do not know\" or row_MLinJob == \"No (we do not use ML methods)\"):\n",
    "                OHE_MLinJob.append(0)\n",
    "            elif(row_MLinJob == \"We are exploring ML methods (and may one day put a model into production)\"):\n",
    "                OHE_MLinJob.append(1)\n",
    "            elif(row_MLinJob == \"We use ML methods for generating insights (but do not put working models into production)\"):\n",
    "                OHE_MLinJob.append(2)\n",
    "            elif(row_MLinJob == \"We recently started using ML methods (i.e., models in production for less than 2 years)\"):\n",
    "                OHE_MLinJob.append(3)\n",
    "            elif(row_MLinJob == \"We have well established ML methods (i.e., models in production for more than 2 years)\"):\n",
    "                OHE_MLinJob.append(4)\n",
    "                \n",
    "        OHE_MLinJob = np.array(OHE_MLinJob)\n",
    "        OHE_MLinJob = OHE_MLinJob.reshape(-1,1)\n",
    "        \n",
    "        print(\"Shape of the OHE_MLinJob: \", OHE_MLinJob.shape)\n",
    "        \n",
    "        return OHE_MLinJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d45d73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordinal Encoding for Spending on ML\n",
    "class SpendingonMLTransformer:\n",
    "    def fit(self, df, y = None, **fit_params):\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df, y = None, **fit_params):\n",
    "        \n",
    "        proportions = df['Spending on ML'].value_counts(normalize=True)\n",
    "        unique_spendingonjob_ranges = proportions.index\n",
    "        df['Spending on ML'] = df['Spending on ML'].fillna(pd.Series(np.random.choice(unique_spendingonjob_ranges,p=proportions,size=len(df))))\n",
    "\n",
    "        #df['Spending on ML'] = df['Spending on ML'].fillna('$0 ($USD)')\n",
    "        \n",
    "        #print(df['Spending on ML'].value_counts(dropna=False))\n",
    "        \n",
    "        OHE_SpendinML = []\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            row_SpendinML = row['Spending on ML']\n",
    "            #print(row_MLinJob)\n",
    "            row_SpendinML = row_SpendinML.strip()\n",
    "            if(row_SpendinML == \"$0 ($USD)\"):\n",
    "                OHE_SpendinML.append(0)\n",
    "            elif(row_SpendinML == \"$100,000 or more ($USD)\"):\n",
    "                OHE_SpendinML.append(100000)\n",
    "            else:\n",
    "                #print(row_SpendinML)\n",
    "                row_SpendinML = row_SpendinML.replace(\"$\",\"\")\n",
    "                row_SpendinML = row_SpendinML.replace(\",\",\"\")\n",
    "                row_SpendinML = row_SpendinML.strip()\n",
    "                lower, upper = row_SpendinML.split(\"-\")\n",
    "                mean_spend = (float(lower)+float(upper))/2\n",
    "                #print(mean_spend)\n",
    "                OHE_SpendinML.append(mean_spend)\n",
    "        \n",
    "        OHE_SpendinML = np.array(OHE_SpendinML)\n",
    "        min_val = np.min(OHE_SpendinML)\n",
    "        max_val = np.max(OHE_SpendinML)\n",
    "        OHE_SpendinML = (OHE_SpendinML - min_val) / (max_val - min_val)\n",
    "        \n",
    "        print(min_val)\n",
    "        print(max_val)\n",
    "        \n",
    "        OHE_SpendinML = OHE_SpendinML.reshape(-1,1)\n",
    "        \n",
    "        print(\"Shape of the OHE_SpendinML: \", OHE_SpendinML.shape)\n",
    "        \n",
    "        return OHE_SpendinML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dfb35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureTransformer = FeatureUnion([\n",
    "    ('Experience', Pipeline([('exp', ExpTransformer())])),\n",
    "    ('Age', Pipeline([('age', AgeTransformer())])),\n",
    "    ('Education', Pipeline([('edu', EduTransformer())])),\n",
    "    ('Programming_Languages', Pipeline([('PL', PLTransformer())])),\n",
    "    ('Country', Pipeline([('Country', CountryTransformer())])),\n",
    "    ('Data_Visualisation', Pipeline([('DV', DataVisTransformer())])),\n",
    "    ('ML_Years', Pipeline([('ML_Years', MLYearsTransformer())])),\n",
    "    ('Gender', Pipeline([('Gender', GenderTransformer())])),\n",
    "    ('ML_in_Job', Pipeline([('ML_in_Job', MLinJobTransformer())])),\n",
    "    ('Spending_on_ML', Pipeline([('Spending_on_ML', SpendingonMLTransformer())]))  \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4836d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransformedData(df):\n",
    "    \n",
    "    print(\"Entered getTransformedData Function\")\n",
    "    \n",
    "    featureTransformer = FeatureUnion([\n",
    "    ('Experience', Pipeline([('exp', ExpTransformer())])),\n",
    "    ('Age', Pipeline([('age', AgeTransformer())])),\n",
    "    ('Education', Pipeline([('edu', EduTransformer())])),\n",
    "    ('Programming_Languages', Pipeline([('PL', PLTransformer())])),\n",
    "    ('Country', Pipeline([('Country', CountryTransformer())])),\n",
    "    ('Data_Visualisation', Pipeline([('DV', DataVisTransformer())])),\n",
    "    ('ML_Years', Pipeline([('ML_Years', MLYearsTransformer())])),\n",
    "    ('Gender', Pipeline([('Gender', GenderTransformer())])),\n",
    "    ('ML_in_Job', Pipeline([('ML_in_Job', MLinJobTransformer())])),\n",
    "    ('Spending_on_ML', Pipeline([('Spending_on_ML', SpendingonMLTransformer())])),\n",
    "    ('Income', Pipeline([('Spending_on_ML', IncomeTransformer())]))\n",
    "    ])\n",
    "    \n",
    "    print(\"FeatureUnion Performed\")\n",
    "    featureTransformer.fit(df)\n",
    "    print(\"FeatureTransformer Fit Performed\")\n",
    "    transformed_data = featureTransformer.transform(df)\n",
    "\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "58da2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserProfile_FV(user_profile):\n",
    "    \n",
    "    user_profile_FV = []\n",
    "    user_profile_FV.append(user_profile[0])\n",
    "    \n",
    "    #Transforming the age in the user profile\n",
    "    user_profile_age = user_profile[1]\n",
    "    min_val = 21\n",
    "    max_val = 75\n",
    "    user_profile_age = (user_profile_age - min_val) / (max_val - min_val)\n",
    "    user_profile_FV.append(user_profile_age)\n",
    "    \n",
    "    #Transforming the education in the user profile\n",
    "    user_profile_edu_df = pd.DataFrame(0,index=range(1), columns=['Education'])\n",
    "    user_profile_edu_df.at[0,'Education'] = user_profile[2].strip()\n",
    "    user_profile_edt = edt.transform(user_profile_edu_df)\n",
    "    user_profile_FV.append(user_profile_edt[0,0])\n",
    "    \n",
    "    #Transforming the PL in the user profile\n",
    "    user_profile_PL_list = user_profile[3]\n",
    "    pl_series = pd.Series(0, index=pl_list)\n",
    "    for pl in user_profile_PL_list:\n",
    "        pl_series[pl] = 1\n",
    "    for item in pl_series:\n",
    "        user_profile_FV.append(item)\n",
    "        \n",
    "    #Transforming the Country in the user profile\n",
    "    user_profile_cn = user_profile[4]\n",
    "    country_series = pd.Series(0, index=unique_country_list)\n",
    "    country_series[user_profile_cn] = 1\n",
    "    for item in country_series:\n",
    "        user_profile_FV.append(item)\n",
    "        \n",
    "    #Transforming the DV in the user profile\n",
    "    user_profile_dv = user_profile[5]\n",
    "    user_profile_FV.append(len(user_profile_dv))\n",
    "    \n",
    "    #Transforming the ML Years in the user profile\n",
    "    user_profile_MLinYears = user_profile[6]\n",
    "    if(user_profile_MLinYears == \"I do not use machine learning methods\"):\n",
    "        user_profile_MLinYears_num = 0\n",
    "    elif(user_profile_MLinYears == \"Under 1 year\"):\n",
    "        user_profile_MLinYears_num = 0.5\n",
    "    elif(user_profile_MLinYears == \"20 or more years\"):\n",
    "        user_profile_MLinYears_num = 20\n",
    "    else:\n",
    "        user_profile_MLinYears = user_profile_MLinYears.replace(\"years\",\"\")\n",
    "        user_profile_MLinYears = user_profile_MLinYears.strip()\n",
    "        lower, upper = user_profile_MLinYears.split(\"-\")\n",
    "        mean_exp = (float(lower)+float(upper))/2\n",
    "        user_profile_MLinYears_num = mean_exp\n",
    "    min_val = 0\n",
    "    max_val = 20\n",
    "    user_profile_MLinYears_num = (user_profile_MLinYears_num - min_val) / (max_val - min_val)\n",
    "    user_profile_FV.append(user_profile_MLinYears_num)\n",
    "        \n",
    "    #Transforming Gender in the user profile\n",
    "    user_profile_Gender = user_profile[7]\n",
    "    gender_series = pd.Series(0, index=gender_list)\n",
    "    print(len(gender_series))\n",
    "    print(gender_series)\n",
    "    gender_series[user_profile_Gender] = 1\n",
    "    print(len(gender_series))\n",
    "    print(gender_series)\n",
    "    for item in gender_series:\n",
    "        user_profile_FV.append(item)\n",
    "    \n",
    "    #Transforming ML in Job in the user profile\n",
    "    user_profile_MLinJob = user_profile[8]\n",
    "    user_profile_MLinJob = user_profile_MLinJob.strip()\n",
    "    if(user_profile_MLinJob == \"I do not know\" or user_profile_MLinJob == \"No (we do not use ML methods)\"):\n",
    "        user_profile_MLinJob_num = 0\n",
    "    elif(user_profile_MLinJob == \"We are exploring ML methods (and may one day put a model into production)\"):\n",
    "        user_profile_MLinJob_num = 1\n",
    "    elif(user_profile_MLinJob == \"We use ML methods for generating insights (but do not put working models into production)\"):\n",
    "        user_profile_MLinJob_num = 2\n",
    "    elif(user_profile_MLinJob == \"We recently started using ML methods (i.e., models in production for less than 2 years)\"):\n",
    "        user_profile_MLinJob_num = 3\n",
    "    elif(user_profile_MLinJob == \"We have well established ML methods (i.e., models in production for more than 2 years)\"):\n",
    "        user_profile_MLinJob_num = 4\n",
    "    user_profile_FV.append(user_profile_MLinJob_num)\n",
    "    \n",
    "    #Transforming Spending on ML in the user profile\n",
    "    user_profile_SpendingonML = user_profile[9]\n",
    "    user_profile_SpendingonML = user_profile_SpendingonML.strip()\n",
    "    if(user_profile_SpendingonML == \"$0 ($USD)\"):\n",
    "        user_profile_SpendingonML_num = 0\n",
    "    elif(user_profile_SpendingonML == \"$100,000 or more ($USD)\"):\n",
    "        user_profile_SpendingonML_num = 100000\n",
    "    else:\n",
    "        user_profile_SpendingonML = user_profile_SpendingonML.replace(\"$\",\"\")\n",
    "        user_profile_SpendingonML = user_profile_SpendingonML.replace(\",\",\"\")\n",
    "        user_profile_SpendingonML = user_profile_SpendingonML.strip()\n",
    "        lower, upper = user_profile_SpendingonML.split(\"-\")\n",
    "        mean_spend = (float(lower)+float(upper))/2\n",
    "        user_profile_SpendingonML_num = mean_spend       \n",
    "    min_val = 0\n",
    "    max_val = 100000\n",
    "    user_profile_SpendingonML_num = (user_profile_SpendingonML_num - min_val) / (max_val - min_val)\n",
    "    user_profile_FV.append(user_profile_SpendingonML_num)          \n",
    "    \n",
    "    \n",
    "    \n",
    "    user_profile_FV = np.array(user_profile_FV)\n",
    "    user_profile_FV = user_profile_FV.reshape(-1,1)\n",
    "    user_profile_FV = user_profile_FV.T\n",
    "    \n",
    "    return user_profile_FV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebd54d",
   "metadata": {},
   "source": [
    "# Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "810ca80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27160, 33)\n",
      "(6791, 33)\n",
      "(27160,)\n",
      "(6791,)\n",
      "Year                                                           2021\n",
      "Age                                                           50-54\n",
      "Gender                                                          Man\n",
      "Income                                              200,000-249,999\n",
      "Country                                    United States of America\n",
      "Experience                                                20+ years\n",
      "Education                                           Master’s degree\n",
      "Job Role                                             Data Scientist\n",
      "Python                                                       Python\n",
      "R                                                                 R\n",
      "SQL                                                             SQL\n",
      "C                                                               NaN\n",
      "C++                                                             NaN\n",
      "Java                                                            NaN\n",
      "Javascript                                                      NaN\n",
      "Julia                                                           NaN\n",
      "Bash                                                           Bash\n",
      "MATLAB                                                          NaN\n",
      "Other Lang                                                      NaN\n",
      "Matplotlib                                              Matplotlib \n",
      "Seaborn                                                    Seaborn \n",
      "Ploty                                                           NaN\n",
      "Ggplot                                            Ggplot / ggplot2 \n",
      "Shiny                                                        Shiny \n",
      "D3                                                              NaN\n",
      "Altair                                                          NaN\n",
      "Bokeh                                                           NaN\n",
      "Geoplotlib                                                      NaN\n",
      "Leaflet                                                         NaN\n",
      "Other Vis                                                       NaN\n",
      "ML Years                                                  4-5 years\n",
      "ML in Job         We use ML methods for generating insights (but...\n",
      "Spending on ML                                         $1000-$9,999\n",
      "Name: 3, dtype: object\n",
      "Entered getTransformedData Function\n",
      "FeatureUnion Performed\n",
      "FeatureTransformer Fit Performed\n",
      "Shape of the OHE_Exp:  (27160, 1)\n",
      "Shape of the OHE_Age:  (27160, 1)\n",
      "21.0\n",
      "75.0\n",
      "Shape of the OHE_Edu:  (27160, 1)\n",
      "Shape of the OHE_PL:  (27160, 11)\n",
      "Shape of the OHE_Country:  (27160, 69)\n",
      "Shape of the OHE_DV:  (27160, 1)\n",
      "0.0\n",
      "20.0\n",
      "Shape of the OHE_MLY:  (27160, 1)\n",
      "Shape of the OHE_Country:  (27160, 3)\n",
      "Shape of the OHE_MLinJob:  (27160, 1)\n",
      "0.0\n",
      "100000.0\n",
      "Shape of the OHE_SpendinML:  (27160, 1)\n",
      "Min Income  500.0\n",
      "Max Income  500000.0\n",
      "(27160, 91)\n",
      "Entered getTransformedData Function\n",
      "FeatureUnion Performed\n",
      "FeatureTransformer Fit Performed\n",
      "Shape of the OHE_Exp:  (6791, 1)\n",
      "Shape of the OHE_Age:  (6791, 1)\n",
      "21.0\n",
      "75.0\n",
      "Shape of the OHE_Edu:  (6791, 1)\n",
      "Shape of the OHE_PL:  (6791, 11)\n",
      "Shape of the OHE_Country:  (6791, 69)\n",
      "Shape of the OHE_DV:  (6791, 1)\n",
      "0.0\n",
      "20.0\n",
      "Shape of the OHE_MLY:  (6791, 1)\n",
      "Shape of the OHE_Country:  (6791, 3)\n",
      "Shape of the OHE_MLinJob:  (6791, 1)\n",
      "0.0\n",
      "100000.0\n",
      "Shape of the OHE_SpendinML:  (6791, 1)\n",
      "Min Income  500.0\n",
      "Max Income  500000.0\n",
      "(6791, 91)\n",
      "[20.          0.57407407  4.          1.          1.          1.\n",
      "  0.          0.          0.          0.          0.          1.\n",
      "  0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          4.\n",
      "  0.225       1.          0.          0.          2.          0.054995\n",
      "  0.44944845]\n"
     ]
    }
   ],
   "source": [
    "#Encoding the X_train and X_test so that they can be used to train and test the model.\n",
    "\n",
    "y_labels = data['Job Role']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "print(X_train.iloc[3])\n",
    "\n",
    "X_train_encoded = getTransformedData(X_train)\n",
    "print(X_train_encoded.shape)\n",
    "\n",
    "X_test_encoded = getTransformedData(X_test)\n",
    "print(X_test_encoded.shape)\n",
    "\n",
    "print(X_train_encoded[3])\n",
    "\n",
    "X_test_file = X_test.iloc[:20]\n",
    "X_test_file = X_test_file.drop('Job Role', axis=1)\n",
    "X_test_file.to_csv(\"test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fa409938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIncomeforRole(Job_Role):\n",
    "\n",
    "    job_role_income = X_train[X_train['Job Role'] == Job_Role]['Income']\n",
    "    print(len(job_role_income))\n",
    "    job_role_income = [x for x in job_role_income if x is not None]\n",
    "    print(len(job_role_income))\n",
    "    income_num = []\n",
    "    for item in job_role_income:\n",
    "        if(item == \"$0-999\"):\n",
    "            income_num.append(500)\n",
    "        elif(item == \"> $500,000\"):\n",
    "            income_num.append(500000)\n",
    "        elif(type(item)=='str'):\n",
    "            item = item.replace(\",\",\"\")\n",
    "            item = item.strip()\n",
    "            lower, upper = item.split(\"-\")\n",
    "            mean_income = (float(lower)+float(upper))/2\n",
    "            income_num.append(mean_income)             \n",
    "                \n",
    "    median_income = statistics.median(income_num)\n",
    "    print(\"The median income for this role: \",median_income)\n",
    "    print(\"Average income is: \", sum(income_num)/len(income_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e97ad04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2144\n",
      "2144\n",
      "The median income for this role:  500.0\n",
      "Average income is:  51678.27868852459\n"
     ]
    }
   ],
   "source": [
    "getIncomeforRole('Manager')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acbe419",
   "metadata": {},
   "source": [
    "# PCA Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "427aa030",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10, random_state=42)\n",
    "pca_transformer = Pipeline([\n",
    "    (\"features\", featureTransformer),\n",
    "    (\"pca\", pca) \n",
    "])\n",
    "\n",
    "def getPCA(df):\n",
    "    transformed_PCA_data = pca.fit_transform(df)\n",
    "    return transformed_PCA_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36021a3",
   "metadata": {},
   "source": [
    "# KNeighbours Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "644e3fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3142394345457223\n"
     ]
    }
   ],
   "source": [
    "kn_model = KNeighborsClassifier()\n",
    "kn_model.fit(X_train_encoded, y_train)\n",
    "kn_result = kn_model.predict(X_test_encoded)\n",
    "kn_accuracy = accuracy_score(y_test, kn_result)\n",
    "print(kn_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "622a2a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After PCA:  0.3068767486379031\n"
     ]
    }
   ],
   "source": [
    "kn_model = KNeighborsClassifier()\n",
    "kn_model.fit(getPCA(X_train_encoded), y_train)\n",
    "kn_result = kn_model.predict(getPCA(X_test_encoded))\n",
    "kn_accuracy = accuracy_score(y_test, kn_result)\n",
    "print(\"After PCA: \", kn_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc5660",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e987dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40156088941245766\n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n"
     ]
    }
   ],
   "source": [
    "rf_classifier_model = RandomForestClassifier(random_state=42)\n",
    "rf_classifier_model.fit(X_train_encoded, y_train)\n",
    "rf_result = rf_classifier_model.predict(X_test_encoded)\n",
    "rf_accuracy = accuracy_score(y_test, rf_result)\n",
    "print(rf_accuracy)\n",
    "print(type(rf_classifier_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f6118217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After PCA:  0.3626859078191724\n"
     ]
    }
   ],
   "source": [
    "rf_classifier_model_pca = RandomForestClassifier(random_state=42)\n",
    "rf_classifier_model_pca.fit(getPCA(X_train_encoded), y_train)\n",
    "rf_result_pca = rf_classifier_model_pca.predict(getPCA(X_test_encoded))\n",
    "rf_accuracy_pca = accuracy_score(y_test, rf_result_pca)\n",
    "print(\"After PCA: \",rf_accuracy_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "47a76973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27160, 91)\n",
      "(6791, 91)\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best parameters: {'max_depth': 25, 'min_samples_leaf': 1, 'min_samples_split': 15, 'n_estimators': 350}\n",
      "Best accuracy: 0.4171575846833579\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,200,350],  # Number of trees in the forest\n",
    "    'max_depth': [20, 23, 25],  # Maximum depth of the tree\n",
    "    'min_samples_split': [10,15, 20],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 3, 5]     # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search_rf = GridSearchCV(estimator=rf_classifier_model, param_grid=param_grid, \n",
    "                           scoring='accuracy', cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "print(X_train_encoded.shape)\n",
    "print(X_test_encoded.shape)\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search_rf.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Get the best parameters and the corresponding accuracy\n",
    "best_params = grid_search_rf.best_params_\n",
    "best_accuracy = grid_search_rf.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1be47bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4146664703283758\n"
     ]
    }
   ],
   "source": [
    "improved_rf_classifier_model = grid_search_rf.best_estimator_\n",
    "improved_rf_result = improved_rf_classifier_model.predict(X_test_encoded)\n",
    "improved_rf_accuracy = accuracy_score(y_test, improved_rf_result)\n",
    "print(improved_rf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f783805e",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bce17cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3922839051686055\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(random_state=42)\n",
    "svm_model.fit(X_train_encoded, y_train)\n",
    "svm_result = svm_model.predict(X_test_encoded)\n",
    "svm_accuracy = accuracy_score(y_test, svm_result)\n",
    "print(svm_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a4c9bb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After PCA 0.38403769695184803\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(random_state=42)\n",
    "svm_model.fit(getPCA(X_train_encoded), y_train)\n",
    "svm_result = svm_model.predict(getPCA(X_test_encoded))\n",
    "svm_accuracy = accuracy_score(y_test, svm_result)\n",
    "print(\"After PCA\", svm_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce26ab",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "65786801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42070387277278753\n"
     ]
    }
   ],
   "source": [
    "gradient_boost_model = GradientBoostingClassifier(random_state=42)\n",
    "gradient_boost_model.fit(X_train_encoded, y_train)\n",
    "gradient_boost_result = gradient_boost_model.predict(X_test_encoded)\n",
    "gradient_boost_accuracy = accuracy_score(y_test, gradient_boost_result)\n",
    "print(gradient_boost_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fc3a9a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After PCA:  0.42070387277278753\n"
     ]
    }
   ],
   "source": [
    "gradient_boost_model_pca = GradientBoostingClassifier(random_state=42)\n",
    "gradient_boost_model_pca.fit(getPCA(X_train_encoded), y_train)\n",
    "gradient_boost_result_pca = gradient_boost_model_pca.predict(getPCA(X_test_encoded))\n",
    "gradient_boost_accuracy_pca = accuracy_score(y_test, gradient_boost_result)\n",
    "print(\"After PCA: \",gradient_boost_accuracy_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "44a66766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   4.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=350; total time=   7.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=15, n_estimators=350; total time=   6.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=20, n_estimators=350; total time=   6.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=3, min_samples_split=10, n_estimators=200; total time=   3.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=3, min_samples_split=15, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=3, min_samples_split=15, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=3, min_samples_split=15, n_estimators=200; total time=   3.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=3, min_samples_split=20, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=3, min_samples_split=20, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=3, min_samples_split=20, n_estimators=200; total time=   4.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=3, min_samples_split=20, n_estimators=350; total time=   7.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=5, min_samples_split=10, n_estimators=350; total time=   6.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=5, min_samples_split=15, n_estimators=350; total time=   6.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=5, min_samples_split=20, n_estimators=200; total time=   3.4s\n",
      "[CV] END max_depth=23, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=23, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   4.8s\n",
      "[CV] END max_depth=23, min_samples_leaf=1, min_samples_split=10, n_estimators=350; total time=   8.9s\n",
      "[CV] END max_depth=23, min_samples_leaf=1, min_samples_split=15, n_estimators=350; total time=   8.5s\n",
      "[CV] END max_depth=23, min_samples_leaf=1, min_samples_split=20, n_estimators=200; total time=   4.0s\n",
      "[CV] END max_depth=23, min_samples_leaf=3, min_samples_split=10, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=23, min_samples_leaf=3, min_samples_split=10, n_estimators=200; total time=   3.8s\n",
      "[CV] END max_depth=23, min_samples_leaf=3, min_samples_split=10, n_estimators=350; total time=   6.5s\n",
      "[CV] END max_depth=23, min_samples_leaf=3, min_samples_split=15, n_estimators=350; total time=   6.2s\n",
      "[CV] END max_depth=23, min_samples_leaf=3, min_samples_split=20, n_estimators=200; total time=   3.5s\n",
      "[CV] END max_depth=23, min_samples_leaf=5, min_samples_split=10, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=23, min_samples_leaf=5, min_samples_split=10, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=23, min_samples_leaf=5, min_samples_split=10, n_estimators=200; total time=   3.3s\n",
      "[CV] END max_depth=23, min_samples_leaf=5, min_samples_split=15, n_estimators=100; total time=   1.6s\n",
      "[CV] END max_depth=23, min_samples_leaf=5, min_samples_split=15, n_estimators=100; total time=   1.6s\n",
      "[CV] END max_depth=23, min_samples_leaf=5, min_samples_split=15, n_estimators=200; total time=   3.3s\n",
      "[CV] END max_depth=23, min_samples_leaf=5, min_samples_split=15, n_estimators=350; total time=   5.6s\n",
      "[CV] END max_depth=23, min_samples_leaf=5, min_samples_split=20, n_estimators=350; total time=   6.5s\n",
      "[CV] END max_depth=25, min_samples_leaf=1, min_samples_split=10, n_estimators=350; total time=   8.3s\n",
      "[CV] END max_depth=25, min_samples_leaf=1, min_samples_split=15, n_estimators=200; total time=   4.5s\n",
      "[CV] END max_depth=25, min_samples_leaf=1, min_samples_split=20, n_estimators=100; total time=   2.1s\n",
      "[CV] END max_depth=25, min_samples_leaf=1, min_samples_split=20, n_estimators=100; total time=   2.0s\n",
      "[CV] END max_depth=25, min_samples_leaf=1, min_samples_split=20, n_estimators=200; total time=   4.3s\n",
      "[CV] END max_depth=25, min_samples_leaf=3, min_samples_split=10, n_estimators=100; total time=   2.0s\n",
      "[CV] END max_depth=25, min_samples_leaf=3, min_samples_split=10, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=25, min_samples_leaf=3, min_samples_split=10, n_estimators=200; total time=   3.8s\n",
      "[CV] END max_depth=25, min_samples_leaf=3, min_samples_split=10, n_estimators=350; total time=   6.4s\n",
      "[CV] END max_depth=25, min_samples_leaf=3, min_samples_split=15, n_estimators=350; total time=   6.4s\n",
      "[CV] END max_depth=25, min_samples_leaf=3, min_samples_split=20, n_estimators=350; total time=   6.6s\n",
      "[CV] END max_depth=25, min_samples_leaf=5, min_samples_split=10, n_estimators=200; total time=   3.3s\n",
      "[CV] END max_depth=25, min_samples_leaf=5, min_samples_split=10, n_estimators=350; total time=   5.9s\n",
      "[CV] END max_depth=25, min_samples_leaf=5, min_samples_split=15, n_estimators=350; total time=   6.0s\n",
      "[CV] END max_depth=25, min_samples_leaf=5, min_samples_split=20, n_estimators=350; total time=   4.8s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_samples_leaf=6, min_samples_split=2, n_estimators=200; total time= 2.6min\n",
      "[CV] END learning_rate=0.1, max_depth=10, min_samples_leaf=6, min_samples_split=2, n_estimators=100; total time= 2.2min\n",
      "[CV] END learning_rate=0.1, max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 6.9min\n",
      "[CV] END learning_rate=0.3, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 5.0min\n",
      "[CV] END learning_rate=0.3, max_depth=20, min_samples_leaf=6, min_samples_split=2, n_estimators=100; total time= 6.3min\n",
      "[CV] END learning_rate=0.5, max_depth=5, min_samples_leaf=6, min_samples_split=2, n_estimators=100; total time= 1.6min\n",
      "[CV] END learning_rate=0.5, max_depth=5, min_samples_leaf=6, min_samples_split=2, n_estimators=200; total time= 3.7min\n",
      "[CV] END learning_rate=0.5, max_depth=10, min_samples_leaf=6, min_samples_split=2, n_estimators=200; total time= 5.3min\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   2.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=350; total time=   7.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=15, n_estimators=200; total time=   4.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=20, n_estimators=100; total time=   1.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=20, n_estimators=100; total time=   1.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=20, n_estimators=200; total time=   3.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=3, min_samples_split=10, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=3, min_samples_split=10, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=3, min_samples_split=10, n_estimators=200; total time=   3.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=3, min_samples_split=15, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=3, min_samples_split=15, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=20, min_samples_leaf=3, min_samples_split=15, n_estimators=200; total time=   3.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=3, min_samples_split=15, n_estimators=350; total time=   6.8s\n",
      "[CV] END max_depth=20, min_samples_leaf=3, min_samples_split=20, n_estimators=350; total time=   7.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=5, min_samples_split=10, n_estimators=350; total time=   6.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=5, min_samples_split=15, n_estimators=200; total time=   3.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=5, min_samples_split=20, n_estimators=100; total time=   1.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=5, min_samples_split=20, n_estimators=100; total time=   1.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=5, min_samples_split=20, n_estimators=200; total time=   3.4s\n",
      "[CV] END max_depth=23, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   2.6s\n",
      "[CV] END max_depth=23, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   2.3s\n",
      "[CV] END max_depth=23, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   4.9s\n",
      "[CV] END max_depth=23, min_samples_leaf=1, min_samples_split=15, n_estimators=100; total time=   2.5s\n",
      "[CV] END max_depth=23, min_samples_leaf=1, min_samples_split=15, n_estimators=200; total time=   5.2s\n",
      "[CV] END max_depth=23, min_samples_leaf=1, min_samples_split=15, n_estimators=350; total time=   8.2s\n",
      "[CV] END max_depth=23, min_samples_leaf=1, min_samples_split=20, n_estimators=350; total time=   6.7s\n",
      "[CV] END max_depth=23, min_samples_leaf=3, min_samples_split=10, n_estimators=200; total time=   3.7s\n",
      "[CV] END max_depth=23, min_samples_leaf=3, min_samples_split=15, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=23, min_samples_leaf=3, min_samples_split=15, n_estimators=100; total time=   2.0s\n",
      "[CV] END max_depth=23, min_samples_leaf=3, min_samples_split=15, n_estimators=200; total time=   3.7s\n",
      "[CV] END max_depth=23, min_samples_leaf=3, min_samples_split=20, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=23, min_samples_leaf=3, min_samples_split=20, n_estimators=100; total time=   1.9s\n",
      "[CV] END max_depth=23, min_samples_leaf=3, min_samples_split=20, n_estimators=200; total time=   3.5s\n",
      "[CV] END max_depth=23, min_samples_leaf=5, min_samples_split=10, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=23, min_samples_leaf=5, min_samples_split=10, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=23, min_samples_leaf=5, min_samples_split=10, n_estimators=200; total time=   3.5s\n",
      "[CV] END max_depth=23, min_samples_leaf=5, min_samples_split=10, n_estimators=350; total time=   5.8s\n",
      "[CV] END max_depth=23, min_samples_leaf=5, min_samples_split=15, n_estimators=350; total time=   5.7s\n",
      "[CV] END max_depth=23, min_samples_leaf=5, min_samples_split=20, n_estimators=350; total time=   6.1s\n",
      "[CV] END max_depth=25, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   5.0s\n",
      "[CV] END max_depth=25, min_samples_leaf=1, min_samples_split=15, n_estimators=100; total time=   2.2s\n",
      "[CV] END max_depth=25, min_samples_leaf=1, min_samples_split=15, n_estimators=100; total time=   2.3s\n",
      "[CV] END max_depth=25, min_samples_leaf=1, min_samples_split=15, n_estimators=200; total time=   4.6s\n",
      "[CV] END max_depth=25, min_samples_leaf=1, min_samples_split=20, n_estimators=100; total time=   2.1s\n",
      "[CV] END max_depth=25, min_samples_leaf=1, min_samples_split=20, n_estimators=100; total time=   2.0s\n",
      "[CV] END max_depth=25, min_samples_leaf=1, min_samples_split=20, n_estimators=200; total time=   4.2s\n",
      "[CV] END max_depth=25, min_samples_leaf=3, min_samples_split=10, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=25, min_samples_leaf=3, min_samples_split=10, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=25, min_samples_leaf=3, min_samples_split=10, n_estimators=200; total time=   4.0s\n",
      "[CV] END max_depth=25, min_samples_leaf=3, min_samples_split=15, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=25, min_samples_leaf=3, min_samples_split=15, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=25, min_samples_leaf=3, min_samples_split=15, n_estimators=200; total time=   3.6s\n",
      "[CV] END max_depth=25, min_samples_leaf=3, min_samples_split=15, n_estimators=350; total time=   6.5s\n",
      "[CV] END max_depth=25, min_samples_leaf=3, min_samples_split=20, n_estimators=350; total time=   6.4s\n",
      "[CV] END max_depth=25, min_samples_leaf=5, min_samples_split=10, n_estimators=350; total time=   5.8s\n",
      "[CV] END max_depth=25, min_samples_leaf=5, min_samples_split=15, n_estimators=200; total time=   3.5s\n",
      "[CV] END max_depth=25, min_samples_leaf=5, min_samples_split=20, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=25, min_samples_leaf=5, min_samples_split=20, n_estimators=200; total time=   3.3s\n",
      "[CV] END max_depth=25, min_samples_leaf=5, min_samples_split=20, n_estimators=350; total time=   4.7s\n",
      "[CV] END learning_rate=0.1, max_depth=5, min_samples_leaf=6, min_samples_split=2, n_estimators=200; total time= 2.6min\n",
      "[CV] END learning_rate=0.1, max_depth=10, min_samples_leaf=6, min_samples_split=2, n_estimators=100; total time= 2.2min\n",
      "[CV] END learning_rate=0.1, max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 6.7min\n",
      "[CV] END learning_rate=0.3, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time= 5.0min\n",
      "[CV] END learning_rate=0.3, max_depth=20, min_samples_leaf=6, min_samples_split=2, n_estimators=100; total time= 6.2min\n",
      "[CV] END learning_rate=0.5, max_depth=5, min_samples_leaf=6, min_samples_split=2, n_estimators=100; total time= 1.5min\n",
      "[CV] END learning_rate=0.5, max_depth=5, min_samples_leaf=6, min_samples_split=2, n_estimators=200; total time= 3.7min\n",
      "[CV] END learning_rate=0.5, max_depth=10, min_samples_leaf=6, min_samples_split=2, n_estimators=200; total time= 5.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 6, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best accuracy: 0.4171575846833579\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,200],   # Number of boosting stages to be run\n",
    "    'learning_rate': [0.1, 0.3, 0.5], # Learning rate shrinks the contribution of each tree\n",
    "    'max_depth': [5, 10, 20],            # Maximum depth of the individual regression estimators\n",
    "    'min_samples_split': [2],    # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [4, 6]      # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(estimator=gradient_boost_model, param_grid=param_grid, \n",
    "                              cv=2, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit GridSearchCV on the data\n",
    "grid_search_gb.fit(X_train_encoded, y_train)\n",
    "\n",
    "# The best parameters found by GridSearchCV\n",
    "print(\"Best Parameters:\", grid_search_gb.best_params_)\n",
    "print(f\"Best accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3aed97ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4139302017375939\n"
     ]
    }
   ],
   "source": [
    "improved_gradient_boost_model = grid_search_gb.best_estimator_\n",
    "improved_gradient_boost_result = improved_gradient_boost_model.predict(X_test_encoded)\n",
    "improved_gradient_boost_accuracy = accuracy_score(y_test, improved_gradient_boost_result)\n",
    "print(improved_gradient_boost_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a65ec",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b9e54497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3893388308054778\n"
     ]
    }
   ],
   "source": [
    "adaboost_model = AdaBoostClassifier(random_state=42)\n",
    "adaboost_model.fit(X_train_encoded, y_train)\n",
    "adaboost_result = adaboost_model.predict(X_test_encoded)\n",
    "adaboost_model_accuracy = accuracy_score(y_test, adaboost_result)\n",
    "print(adaboost_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b469fcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After PCA:  0.42070387277278753\n"
     ]
    }
   ],
   "source": [
    "adaboost_model_pca = AdaBoostClassifier(random_state=42)\n",
    "adaboost_model_pca.fit(getPCA(X_train_encoded), y_train)\n",
    "adaboost_model_pca = adaboost_model_pca.predict(getPCA(X_test_encoded))\n",
    "adaboost_model_accuracy_pca = accuracy_score(y_test, gradient_boost_result)\n",
    "print(\"After PCA: \",adaboost_model_accuracy_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d4aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
