{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52fj0bRuXgvu"
   },
   "source": [
    "# Assignment 2: SoC Module Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjjJg5d_Xgvw"
   },
   "source": [
    "## Instructions to submit the assignment\n",
    "\n",
    "- Name your jupyter notebook as `Assignment2_[StudentID].ipynb`. For instance: `Assignment2_A0123873A.ipynb`\n",
    "- Your solution notebook must contain the python code that we can run to verify the answers.\n",
    "  - late within 1 hour: 10% reduction in grade\n",
    "  - late within 6 hours: 30% reduction in grade\n",
    "  - late within 12 hours: 50% reduction in grade\n",
    "  - late within 1 days: 70% reduction in grade\n",
    "  - after 1 days: zero mark\n",
    "- **This is an individual assessment. Refrain from working in groups.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTnGjmPVXgvy"
   },
   "source": [
    "In this assignment we design a recomendation engine (*Don't worry about the effectiveness of the system. It maybe very bad. The idea is just to offer you a proof of concept!*). The recommendation engine suggests the students a module that closely matches the modules already taken by the student. The dataset comprices of two files:\n",
    "- List of modules in the School of Computing \n",
    "- List of graduated students and the modules they had taken during their studies\n",
    "\n",
    "Schematic diagram of the entire process is shown as below:\n",
    "\n",
    "![Schema]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgL9CZ-pXgvz"
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Tjl54HB5Xgvz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "'''\n",
    "    YOU MUST USE THE RANDOM SEED WHEREVER NEEDED OR RANDOM_STATE as 42.\n",
    "'''\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "courses = pd.read_csv(\"courses.tsv\", sep='\\t')\n",
    "students = pd.read_csv(\"students.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKrejjTxXgv0"
   },
   "source": [
    "# Question 1: Creating the preprocessing pipeline (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQ_EU5o9Xgv1"
   },
   "source": [
    "We want to create a sklearn pipeline to efficiently preprocess the data and prepare it for training a model. We use three different features in the `courses` data: `specialisation`, `info` and `workload`. We want to represent every feature in a numeric form and merge them to form a feature vector for every course. We do so in the following way:\n",
    "- `specialisation` represents one of the six levels of the module. For instance: CS2103 is a Software Engineering (SE) specialisation module. Encode this categorical feature into a vector. The decision of handling missing values is left to you! *(Hint: You can use `MultiLabelBinerizer` to do so.)*\n",
    "- `info` provides a short discription of the module. We want to convert it into a vector using CountVectorizer. *Don't forget to remove the stopwords* while doing so.\n",
    "-  `workload` states the intended distribution of workload over lectures, tutorials, labs and self study. We want to find the workload as the sum of individual workloads. For instnce: 3-1-1-3-2 workload transforms to 10 hours.\n",
    "\n",
    "Provide implementation for three classes that help us build the pipeline. `transformed_courses` should be a numpy array of shape `[n_courses X n_features]`.\n",
    "\n",
    "                                                                                                   (6 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing \"Netoworking\" typo with Networking\n",
    "courses['specialisation'] = courses['specialisation'].replace(\"Netoworking\",\"Networking\")\n",
    "\n",
    "#Filling the NAN values with relative proportions of the existing values in that column\n",
    "proportions = courses['specialisation'].value_counts(normalize=True)\n",
    "#print(proportions)\n",
    "unique_specialisations = proportions.index\n",
    "courses['specialisation'] = courses['specialisation'].fillna(pd.Series(np.random.choice(unique_specialisations,p=proportions,size=len(courses))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sVBlKTN6Xgv1"
   },
   "outputs": [],
   "source": [
    "class WorkloadTransformer:        \n",
    "    def fit(self, X, y = None, **fit_params):\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def transform(self, X, y = None, **fit_params):\n",
    "        def compute_wkld(x):\n",
    "            x = x.strip()\n",
    "            str_list = x.split(\"-\")\n",
    "            workloads = [float(element) for element in str_list]\n",
    "            total_workload = sum(workloads)\n",
    "            return total_workload\n",
    "        \n",
    "        result = X['workload'].apply(compute_wkld)\n",
    "        result = result.to_numpy()\n",
    "        #print(result.shape)\n",
    "        result = result.reshape(-1, 1)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EZ_fhJkFXgv2"
   },
   "outputs": [],
   "source": [
    "class InfoTransformer:  \n",
    "    def fit(self, X, y = None, **fit_params):\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def transform(self, X, y = None, **fit_params):\n",
    "        cv = CountVectorizer(stop_words='english')\n",
    "        cv.fit(X['info'])\n",
    "        count_vector = cv.transform(X['info'])\n",
    "        \n",
    "        return count_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oLKGAPR1Xgv3"
   },
   "outputs": [],
   "source": [
    "class SpecTransformer:        \n",
    "    def fit(self, X, y = None, **fit_params):\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def transform(self, X, y = None, **fit_params):\n",
    "        each_course_specialisations = {}\n",
    "        specialisation_list = []\n",
    "\n",
    "        for index, row in X.iterrows():\n",
    "            each_course_specialisations = {x.strip() for x in row['specialisation'].split(\",\")}\n",
    "            specialisation_list.append(each_course_specialisations)\n",
    "\n",
    "        #print(specialisation_list)\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        mlb.fit(specialisation_list)\n",
    "        \n",
    "        \n",
    "        OHE_Specialisation = mlb.transform(specialisation_list)\n",
    "        \n",
    "        unique_labels = mlb.classes_\n",
    "        label_indices = {label: index for index, label in enumerate(unique_labels)}\n",
    "        #print(label_indices)\n",
    "        \n",
    "        return OHE_Specialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kcx2rx6CXgv4"
   },
   "outputs": [],
   "source": [
    "featureTransformer = FeatureUnion([\n",
    "    ('workload_processing', Pipeline([('wrkld', WorkloadTransformer())])),\n",
    "    ('info_processing', Pipeline([('info', InfoTransformer())])),\n",
    "    ('spec_processing', Pipeline([('spec', SpecTransformer())])),\n",
    "])\n",
    "\n",
    "featureTransformer.fit(courses)\n",
    "transformed_courses = featureTransformer.transform(courses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xtc4ObIlXgv5"
   },
   "source": [
    "Now we prepare our testing data in the same way we preprocessed the course. `students` data comprises of 1000 students and a list of modules they have taken. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQu6wIgYXgv6"
   },
   "source": [
    "Create `Xtest` and `Ytest` as two matrices. `Xtest`, of size `1000*5`, comprises of first five modules for every student in the list. `Ytest`, of size `1000*[remaining_modules]`, comprises of rest of the modules for every student in the list. \n",
    "We do so in order to assess the performance of the recommender. We assess the recommender based on its effectiveness to predict the modules given a list of five modules as the input.\n",
    "\n",
    "For instance: \n",
    "- `Xtest[0] = ['CS2105', 'CS4222', 'CS6270', 'CS6205', 'CS4226']`\n",
    "- `Ytest[0] = ['CS3282', 'CS6204', 'CS5223', 'CS3281', 'CS4344', 'CS5422', 'CS3237', 'CS5233']`.\n",
    "\n",
    "<div align=\"right\">(2 marks)</align>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "R2t2xUhYXgv6"
   },
   "outputs": [],
   "source": [
    "students_courses = students['courses']\n",
    "Xtest = []\n",
    "Ytest = []\n",
    "\n",
    "for element in students_courses:\n",
    "    student_course_list = element.split(\",\")\n",
    "    Xtest.append(student_course_list[:5])\n",
    "    Ytest.append(student_course_list[5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8-D7LhvXgv7"
   },
   "source": [
    "For every student in `Xtest`, we need to transform the list of 5 modules to the feature space using the `featureTransformer` fit on the training data. For every module we will get a feature vector of size `n_features`. We *add* these feature vectors to get an aggregate feature vector for very student.\n",
    "\n",
    "Write a function `getFeatureVector` that takes in the list of modules and `featureTransformer`. It returns the feature vector for the specified list of courses. For instance, `getFeatureVector(Xtest[0], featureTransformer)` will return a vector of size `n_features`.\n",
    "\n",
    "<div align=\"right\">(2 marks)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1FaBqoBPXgv7"
   },
   "outputs": [],
   "source": [
    "def getFeatureVector(modules, featureTransformer):\n",
    "    modules_in_courses = courses[courses['code'].isin(modules)]\n",
    "    modules_index_courses = modules_in_courses.index\n",
    "    transformed_courses = featureTransformer.transform(courses)\n",
    "    modules_featurevector = transformed_courses[modules_index_courses]\n",
    "    aggregated_featurevector = np.sum(modules_featurevector, axis=0)\n",
    "    \n",
    "    return aggregated_featurevector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CS5422', 'CS5223', 'CS4237', 'CS3281', 'CS6213']\n",
      "[50.  0.  0. ...  3.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(Xtest[0])\n",
    "result = getFeatureVector(Xtest[0],featureTransformer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuwxpWB2Xgv7"
   },
   "source": [
    "# Question 2: Content based recommender (4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJbpI-xdXgv8"
   },
   "source": [
    "We can use a model as simple as K-nearest neighbour (KNN) to perform a content based recommendation. If we provide a list of 5 modules to the recommender, it provide us a list of modules that are similar to the specified modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQEMNzrcXgv8"
   },
   "source": [
    "`sklearn` provides `NearestNeighbors` as well as `KNeighborsClassifier`, both of which have a similar functionality. `NearestNeighbors` provides as an easy functionality to predict a list of K nearest neighbours. Therefore, we prefer it over `KNeighborsClassifier`. If we want to find K nearest points to a datapoint`d`, we need to use `n_neighbors` as K + 1 because the list includes `d` itself.\n",
    "\n",
    "You can now train the model using the training data, which comprises of `transformed_courses` and with their codes as the labels. \n",
    "<div align=\"right\">(1 mark)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ezqrA0XsXgv8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(algorithm=&#x27;brute&#x27;, n_neighbors=6)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(algorithm=&#x27;brute&#x27;, n_neighbors=6)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(algorithm='brute', n_neighbors=6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 5\n",
    "model = NearestNeighbors(algorithm=\"brute\", n_neighbors = K + 1)\n",
    "model.fit(transformed_courses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_Oo47yaXgv9"
   },
   "source": [
    "It is time to see our model in action. Let's see what modules our model reommends based on the modules taken by a student.\n",
    "\n",
    "Write a function that takes in a *pre-trained* model of your choice as input and the list of modules. It returns the top-K recommendations of the model. Print the top 6 recommendations for the first student. \n",
    "<div align=\"right\">(3 marks)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wH0yg5I6Xgv9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 6 recommendations for the first student\n",
      "33     CS3203\n",
      "34     CS3205\n",
      "112    CS5223\n",
      "12     CS2020\n",
      "38     CS3216\n",
      "39     CS3217\n",
      "Name: code, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def recommend(model, modulesTaken, k = 6):\n",
    "    \n",
    "    aggregated_featurevector = getFeatureVector(modulesTaken, featureTransformer)\n",
    "    aggregated_featurevector = aggregated_featurevector.reshape(-1,1)\n",
    "    aggregated_featurevector = aggregated_featurevector.T\n",
    "    result = model.kneighbors(aggregated_featurevector, return_distance=False)\n",
    "    result_indices = result[0]\n",
    "    recommended_courses = courses.iloc[result_indices]['code']\n",
    "    #print(len(recommended_courses))\n",
    "    \n",
    "    return recommended_courses[:k]\n",
    "\n",
    "print(\"Top 6 recommendations for the first student\")\n",
    "print(recommend(model, Xtest[0], 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVVhPPoPXgv9"
   },
   "source": [
    "# Question 3: Recommender evaluation (6 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrxDQQWCXgv-"
   },
   "source": [
    "Is this the model any good? To assess the performance of the model, we use **precision** and **recall** as our metrics. `Ytest` consists of true labels for every students. Using those labels as the ground truth, compute the precision and recall for every student. Write a code that prints values of average precision and recall for a specific value of `K` over the `students` dataset. Print the value of average precision and average recall for `K= 10`.\n",
    "\n",
    "                                                                                                     (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IPyZswBBXgv-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision:  0.050166666666666665\n",
      "Average Recall:  0.02938390091160989\n"
     ]
    }
   ],
   "source": [
    "precision_list = []\n",
    "recall_list = []\n",
    "for i in range(0,len(Xtest)):\n",
    "    predicted_courses = recommend(model, Xtest[i], 10)\n",
    "    #print(\"Length of the predicted courses: \", len(predicted_courses))\n",
    "    predicted_courses = set(predicted_courses)\n",
    "    actual_courses = set(Ytest[i])\n",
    "    intersection = predicted_courses.intersection(actual_courses)\n",
    "    \n",
    "    precision_i = len(intersection)/len(predicted_courses)\n",
    "    recall_i = len(intersection)/len(actual_courses)\n",
    "    \n",
    "    precision_list.append(precision_i)\n",
    "    recall_list.append(recall_i)\n",
    "    \n",
    "print(\"Average Precision: \",np.mean(precision_list))\n",
    "print(\"Average Recall: \",np.mean(recall_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeR2w7IpXgv-"
   },
   "source": [
    "We observe that both precision and recall is not really great. The reason might be igh feature dimension, which may even be noisy. Append the exisiting `featureTransformer` with a PCA to reduce the dimension. \n",
    "\n",
    "Print the value of average precision and recall for `K= 10` after the introduction of PCA.\n",
    "\n",
    "                                                                                                     (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HNr2Tb56Xgv_"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(algorithm=&#x27;brute&#x27;, n_neighbors=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(algorithm=&#x27;brute&#x27;, n_neighbors=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(algorithm='brute', n_neighbors=10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=30, random_state=42)\n",
    "pca_transformer = Pipeline([\n",
    "    (\"features\", featureTransformer),\n",
    "    (\"pca\", pca) \n",
    "])\n",
    "transformed_PCA_data = pca.fit_transform(transformed_courses)\n",
    "model_pca = NearestNeighbors(algorithm = \"brute\", n_neighbors = 10)\n",
    "model_pca.fit(transformed_PCA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_pca(model_pca, modulesTaken, k = 10):\n",
    "    aggregated_FV_PCA = getFeatureVector(modulesTaken, pca_transformer)\n",
    "    aggregated_FV_PCA = aggregated_FV_PCA.reshape(-1,1)\n",
    "    aggregated_FV_PCA = aggregated_FV_PCA.T\n",
    "    result = model_pca.kneighbors(aggregated_FV_PCA, return_distance=False)\n",
    "    result_indices = result[0]\n",
    "    recommended_courses = courses.iloc[result_indices]['code']\n",
    "    return recommended_courses[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision after PCA:  0.14110000000000003\n",
      "Average Recall after PCA:  0.149061360573551\n"
     ]
    }
   ],
   "source": [
    "precision_list = []\n",
    "recall_list = []\n",
    "for i in range(0,len(Xtest)):\n",
    "    predicted_courses = recommend_pca(model_pca, Xtest[i], 10)\n",
    "    predicted_courses = set(predicted_courses)\n",
    "    actual_courses = set(Ytest[i])\n",
    "    intersection = predicted_courses.intersection(actual_courses)\n",
    "    \n",
    "    precision_i = len(intersection)/len(predicted_courses)\n",
    "    recall_i = len(intersection)/len(actual_courses)\n",
    "    \n",
    "    precision_list.append(precision_i)\n",
    "    recall_list.append(recall_i)\n",
    "    \n",
    "print(\"Average Precision after PCA: \",np.mean(precision_list))\n",
    "print(\"Average Recall after PCA: \",np.mean(recall_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ih2GatUQXgv_"
   },
   "source": [
    "Can you provide some **concrete** (something that you can implement) suggestions to improve the performance of the system? The improvement does not have to be very significant.\n",
    "\n",
    "                                                                                                    (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following two suggestions have been implemented below,\n",
    "\n",
    "1.) Setting min_df=2 helps filter out very rare terms that may be typos, noise, or specific to a small subset of documents. Can reduce the dimensionality of the feature space, which can lead to faster training times and potentially better model generalization.\n",
    "\n",
    "Setting max_df=0.95 so as to filter out common terms that has very less discriminatory power thereby improving the informativeness of the features by focusing on more specific terms.\n",
    "\n",
    "2.) The specialisation to which a specific course belongs, will be a better determinant for predicting which course will potentially be chosen by the student in the future. So, more weightage can be given to Specialisation feature than the other two. I have assigned a weigtage of 0.8 to Specialisation and 0.1 to Workload and Info each. \n",
    "\n",
    "Upon implementing the above two improvements, we get an increase of around 5% in both average precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "XzTUXzCnXgv_"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "class InfoTransformer_improved:  \n",
    "    def fit(self, X, y = None, **fit_params):\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def transform(self, X, y = None, **fit_params):\n",
    "        cv = CountVectorizer(min_df=2, max_df=0.95, stop_words='english')\n",
    "        cv.fit(X['info'])\n",
    "        count_vector = cv.transform(X['info'])\n",
    "        \n",
    "        return count_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureTransformer = FeatureUnion([\n",
    "    ('workload_processing', Pipeline([('wrkld', WorkloadTransformer())])),\n",
    "    ('info_processing_improved', Pipeline([('info_improved', InfoTransformer_improved())])),\n",
    "    ('spec_processing', Pipeline([('spec', SpecTransformer())])),\n",
    "],transformer_weights={'workload_processing': 0.1, 'info_processing_improved': 0.1, 'spec_processing': 0.8})\n",
    "\n",
    "\n",
    "featureTransformer.fit(courses)\n",
    "transformed_courses = featureTransformer.transform(courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(algorithm=&#x27;brute&#x27;, n_neighbors=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(algorithm=&#x27;brute&#x27;, n_neighbors=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(algorithm='brute', n_neighbors=10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=30, random_state=42)\n",
    "pca_transformer = Pipeline([\n",
    "    (\"features\", featureTransformer),\n",
    "    (\"pca\", pca) \n",
    "])\n",
    "transformed_PCA_data = pca.fit_transform(transformed_courses)\n",
    "model_pca = NearestNeighbors(algorithm = \"brute\", n_neighbors = 10)\n",
    "model_pca.fit(transformed_PCA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_pca(model_pca, modulesTaken, k = 10):\n",
    "    aggregated_FV_PCA = getFeatureVector(modulesTaken, pca_transformer)\n",
    "    aggregated_FV_PCA = aggregated_FV_PCA.reshape(-1,1)\n",
    "    aggregated_FV_PCA = aggregated_FV_PCA.T\n",
    "    result = model_pca.kneighbors(aggregated_FV_PCA, return_distance=False)\n",
    "    result_indices = result[0]\n",
    "    recommended_courses = courses.iloc[result_indices]['code']\n",
    "    return recommended_courses[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision after PCA and improvement:  0.19390000000000004\n",
      "Average Recall after PCA and improvement :  0.20554380000374584\n"
     ]
    }
   ],
   "source": [
    "precision_list = []\n",
    "recall_list = []\n",
    "for i in range(0,len(Xtest)):\n",
    "    predicted_courses = recommend_pca(model_pca, Xtest[i], 10)\n",
    "    predicted_courses = set(predicted_courses)\n",
    "    actual_courses = set(Ytest[i])\n",
    "    intersection = predicted_courses.intersection(actual_courses)\n",
    "    \n",
    "    precision_i = len(intersection)/len(predicted_courses)\n",
    "    recall_i = len(intersection)/len(actual_courses)\n",
    "    \n",
    "    precision_list.append(precision_i)\n",
    "    recall_list.append(recall_i)\n",
    "    \n",
    "print(\"Average Precision after PCA and improvement: \",np.mean(precision_list))\n",
    "print(\"Average Recall after PCA and improvement : \",np.mean(recall_list))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
